{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2. Exploración y Curación de Datos.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<small>Mentoría de la Diplomatura en Ciencia de Datos y sus Aplicaciones - FaMAF UNC 2022.</small>\n",
        "# Detección de anomalías en despachos de combustible\n",
        "\n",
        "## Práctico de Exploración y Curación de Datos\n",
        "\n",
        "**Mentora:** Daniela Bosch\n",
        "\n",
        "**Integrantes:**\n",
        "\n",
        "### Introducción\n",
        "\n",
        "En este trabajo realizamos una exploración en mayor profundidad y dejamos el dataset curado para los próximos trabajos.\n",
        "\n",
        "Los datos con la descripción de cada una de sus columnas puede encontrarse en el siguiente link: https://www.kaggle.com/datasets/danielabosch/fuel-transactions-from-gas-pump\n",
        "\n",
        "#### Objetivos del práctico\n",
        "\n",
        "- Observar datos ruidosos. Diferenciar correctamente los faltantes de los anómalos o mal codificados.\n",
        "- Obtener un dataset curado para ser utilizado en los próximos trabajos.\n"
      ],
      "metadata": {
        "id": "djgok9GUdaNl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Consignas\n",
        "\n",
        "Se proponen cuatro actividades de base y dos actividades opcionales. Realizar una breve conclusión sobre el dataset generado."
      ],
      "metadata": {
        "id": "eJGu722BBBzG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Análisis de datos faltantes y limpieza\n",
        "\n",
        "- Realizar un análisis de datos faltantes. Tener en cuenta estos datos pueden presentarse también de formas distintas de acuerdo a la variable (ej. \"Producto 0\" en la columna de `nombre_prod`).\n",
        "- Decidir qué columnas se imputarán y cuáles se descartarán.\n",
        "- Descartar las columnas que no se utilizarán para detectar las anomalías.\n",
        "- Descartar (si corresponden) las filas que no se consideran parte de este proyecto al tratarse de otro tipo de transacciones (ej. abastecimientos).\n",
        "\n",
        "> _Muchos valores que pueden considerarse como nulos o faltantes pueden en realidad tratarse de datos erróneos. Tener en cuenta esto al momento de analizarlos y luego imputarlos_."
      ],
      "metadata": {
        "id": "W9q5SEhaBD3y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6Vs8LiFI0McO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Transformación de datos\n",
        "\n",
        "- Realizar un encoding tipo One Hot de las columnas seleccionadas que no contengan valores nulos.\n",
        "- Escalar, estandarizar, normalizar los datos si es necesario.\n",
        "- (Opcional) Se recomienda el uso de Pipelines para realizar estas transformaciones. "
      ],
      "metadata": {
        "id": "ZKyAs__HA-Tc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "i1Y9li5DSwIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Imputación de datos faltantes\n",
        "\n",
        "- En el caso de haber seleccionado algunas de las columnas con valores faltantes, imputar sus valores con el método y estimador convenientes para poder ser utilizados.\n",
        "- Realizar el encoding (si corresponde) sobre las columnas imputadas."
      ],
      "metadata": {
        "id": "jSh8Lx1CGtx_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "KXRmmWltHCoI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Guardado del dataset\n",
        "\n",
        "Luego de realizar las transformaciones generar los siguientes archivos:\n",
        "- .csv o .pickle con el dataset transformado en el punto 2 que será utilizado en el último práctico.\n",
        "- .csv o .pickle con el dataset transformado **sin la tranformación de la columna `codigo_error` one hot (si fue incluida en la anterior)**.\n",
        "- .csv o .pickle con la columna `codigo_error` (o incluirse como una columna extra en el archivo anterior) que será utilizado como label en el próximo práctico.\n",
        "\n",
        "**Input**: .csv completo.  \n",
        "**Output**: Uno a tres .csv/.pickle limpios y transformados (según transformaciones realizadas)."
      ],
      "metadata": {
        "id": "CQh0RcwOYuJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ZHcWOPUWBBUv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (Opcional) 5. Reducción de dimensionalidad usando PCA\n",
        "\n",
        "- Aplicar PCA para reducir las dimensiones del dataset transformado.\n",
        "- Ahora realizar el mismo procedimiento seleccionando sólo una o dos [  empresas | industrias | productos ].\n",
        "- Graficar ambos resultados (2 o 3 componentes), observar la dispersión de los datos en el nuevo espacio. \n",
        "\n",
        "> _Este proceso puede ser muy lento. A fines de observación puede reducirse el dataset a una semana o un mes._\n",
        "\n",
        "https://medium.com/@ansjin/dimensionality-reduction-using-pca-on-multivariate-timeseries-data-b5cc07238dc4"
      ],
      "metadata": {
        "id": "g_2cJ-I_QPQL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "pca = PCA()\n",
        "# X_pca = pca.fit(X)"
      ],
      "metadata": {
        "id": "zkMIyrmO2Jks"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### (Opcional) 6. Medias y medianas móviles para observar ouliers\n",
        "\n",
        "- Graficar las cantidades de las transacciones de una o dos empresas junto a la media móvil y otra con la mediana móvil. Identificar outliers.\n",
        "- Graficar las cantidades acumuladas de las transacciones de una o dos empresas junto a la media móvil y otra con la mediana móvil del acumulado. Identificar outliers.\n",
        "- Graficar la media y mediana móvil del residuo de ambas series descompuestas. Identificar outliers.\n",
        "- **¿Qué resultados se obtienen observando los outliers utilizando medias móviles? ¿Qué resultados se obtienen con la mediana móvil? ¿Cuál funciona mejor y por qué?**\n",
        "\n",
        "https://anomaly.io/anomaly-detection-moving-median-decomposition/index.html\n",
        "\n"
      ],
      "metadata": {
        "id": "YTyrdDJKdzKF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Para poder utilizarse un window en tiempo, es necesario que el index sea de tiempo.\n",
        "# df.rolling('15T')[\"cantidad\"].mean()"
      ],
      "metadata": {
        "id": "5oKkWz3L0N80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Entregable\n",
        "\n",
        "Se propone la elaboración de un informe que deberá entregarse en formato de notebook tipo Jupyter o Google Colaboratory, con los resultados guardados, gráficos, notas y conclusiones. El mismo deberá apuntar a un público técnico, pero sin conocimientos específicos en el tema.\n",
        "\n",
        "**Fecha de entrega: 17/06/2022**"
      ],
      "metadata": {
        "id": "lES1-wl6EAMv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Enlaces útiles\n",
        "- https://medium.com/@ansjin/dimensionality-reduction-using-pca-on-multivariate-timeseries-data-b5cc07238dc4\n",
        "- https://anomaly.io/anomaly-detection-moving-median-decomposition/index.html\n",
        "- https://neptune.ai/blog/anomaly-detection-in-time-series"
      ],
      "metadata": {
        "id": "cKL75SIKGilo"
      }
    }
  ]
}